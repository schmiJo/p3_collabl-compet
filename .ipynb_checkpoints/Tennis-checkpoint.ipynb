{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make imports and create the agent and the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "env = UnityEnvironment(file_name=\"./Tennis.app\")\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])\n",
    "\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from agent import AgentList\n",
    "\n",
    "agents = AgentList(state_size= 24, action_size=2, agents_count = 2, random_seed = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 1\tavg Score: 0.80\n",
      "Environment solved in 1 episodes!\tAverage Score: 0.80\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def ddpg(n_episodes: int =  6000, max_actions:int = 1500 ):\n",
    "    scores = []\n",
    "    scores_window= deque(maxlen=100)\n",
    "    \n",
    "    \n",
    "    for i_episode in range(1,n_episodes+1):\n",
    "        \n",
    "        #reset the environment for every episode and initialize the state\n",
    "        #step_brain_info = env.reset(train_mode=i_episode % 100 != 0)[brain_name] # reset the environment\n",
    "        step_brain_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        \n",
    "        agents.reset_noise()\n",
    "        \n",
    "        states = step_brain_info.vector_observations   #obtain the starting state\n",
    "        \n",
    "        score_0 = 0 #initialze the score for the episode\n",
    "        score_1 = 0 #initialze the score for the episode\n",
    "        \n",
    "        \n",
    "        for a in range(max_actions):\n",
    "            \n",
    "            #The agent chooses an action\n",
    "            # after 200 episodes the actor should not add noise to the action to create a more stable agent\n",
    "            actions = agents.act(states, add_noise=i_episode < 1000)\n",
    "            \n",
    "            step_brain_info = env.step(actions)[brain_name]\n",
    "            \n",
    "            next_states: np.ndarray = step_brain_info.vector_observations   # get the next state\n",
    "            rewards = step_brain_info.rewards                   # get the reward\n",
    "            dones = step_brain_info.local_done                  # see if episode has finished\n",
    "            \n",
    "            score_0 += rewards[0]\n",
    "            score_1 += rewards[1]\n",
    "            \n",
    "            #Take a step and learn from the s,a,r,s,d pair\n",
    "            agents.step(states, actions, rewards, next_states, dones)\n",
    "             \n",
    "            states = next_states                             # roll over the state to next time step\n",
    "            if agents.is_all_done(dones):                             # exit loop if episode finished\n",
    "                break\n",
    "                \n",
    "            if env.global_done:\n",
    "                print('Global Done Steps: '+ str(a))\n",
    "                break\n",
    "        \n",
    "        bigger_score = max([score_0, score_1])\n",
    "        scores.append(bigger_score)\n",
    "        scores_window.append(bigger_score)       # save most recent score\n",
    "\n",
    "        \n",
    "        print('\\rEpisode {}\\tavg Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        \n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tavg Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window)>=0.5:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            #torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "            \n",
    "    \n",
    "    return scores\n",
    "            \n",
    "    \n",
    "scores =  ddpg()\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the weights\n",
    "agents.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See the trained agent in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore weights\n",
    "\n",
    "# agents.restore_state()\n",
    "\n",
    "n_episodes = 2\n",
    "for i_episode in range(1,n_episodes+1):\n",
    "      #reset the environment for every episode and initialize the state\n",
    "    #step_brain_info = env.reset(train_mode=i_episode % 100 != 0)[brain_name] # reset the environment\n",
    "    step_brain_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "        \n",
    "    agents.reset_noise()\n",
    "        \n",
    "    states = step_brain_info.vector_observations   #obtain the starting state\n",
    "        \n",
    "    score_0 = 0 #initialze the score for the episode\n",
    "    score_1 = 0 #initialze the score for the episode\n",
    "        \n",
    "        \n",
    "    for a in range(5000):\n",
    "            \n",
    "        #The agent chooses an action\n",
    "        # after 200 episodes the actor should not add noise to the action to create a more stable agent\n",
    "        actions = agents.act(states, add_noise=i_episode < 1000)\n",
    "            \n",
    "        step_brain_info = env.step(actions)[brain_name]\n",
    "            \n",
    "        next_states: np.ndarray = step_brain_info.vector_observations   # get the next state\n",
    "        rewards = step_brain_info.rewards                   # get the reward\n",
    "        dones = step_brain_info.local_done                  # see if episode has finished\n",
    "            \n",
    "        score_0 += rewards[0]\n",
    "        score_1 += rewards[1]\n",
    "        \n",
    "        #Take a step and learn from the s,a,r,s,d pair\n",
    "        agents.step(states, actions, rewards, next_states, dones)\n",
    "         \n",
    "        states = next_states                             # roll over the state to next time step\n",
    "        if agents.is_all_done(dones):                             # exit loop if episode finished\n",
    "            break\n",
    "            \n",
    "        if env.global_done:\n",
    "            print('Global Done Steps: '+ str(a))\n",
    "            break\n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
